{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "# author Zhang Jun 2021 (with some later modfications)\n",
    "# Example how to preprocess raw text data. This is not complete/optimized,\n",
    "# but you can you use the code as a skeleton for your own program.\n",
    "\n",
    "# To be able to run this code, make sure you have the packages NLTK and scikit-learn installed.\n",
    "# If not, you can install them with pip via command \"pip3 install scikit-learn nltk\"\n",
    "# Load the required packages that are used in this example.\n",
    "\n",
    "# Edit / Kerkko: added nltk downloads to run script. Our text list is now the list of titles and abstracts."
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-24T13:03:39.993582200Z",
     "start_time": "2023-11-24T13:03:39.985552300Z"
    }
   },
   "id": "789c247284e5c75"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\iliaz\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\iliaz\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-24T13:39:24.199863100Z",
     "start_time": "2023-11-24T13:39:24.180477400Z"
    }
   },
   "id": "cb03ff139142d133"
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First documents:\n",
      "Anomaly detection in wide area imagery  This study is about detecting anomalies in wide area imagery collected from an aircraft. The set of anomalies have been identified as anything out of the normal course of action. For this purpose, two different data sets were used and the experiments were carried out on these data sets. For anomaly detection, a convolutional neural network model that tries to generate the next image using past images is designed. The images were pre-processed before being given to the model. Anomaly detection is performed by comparing the estimated image and the true image. \n",
      "\n",
      "Person re-identification with deep kronecker-product matching and group-shuffling random walk Person re-identification (re-ID) aims to robustly measure visual affinities between person images. It has wide applications in intelligent surveillance by associating same persons' images across multiple cameras. It is generally treated as an image retrieval problem: Given a probe person image, the affinities between the probe image and gallery images (P2G affinities) are used to rank the retrieved gallery images. There exist two main challenges for effectively solving this problem. 1) Person images usually show significant variations because of different person poses and viewing angles. The spatial layouts and correspondences between person images are therefore vital information for tackling this problem. State-of-the-art methods either ignore such spatial variation or utilize extra pose information for handling the challenge. 2) Most existing person re-ID methods rank gallery images considering only P2G affinities but ignore the affinities between the gallery images (G2G affinity). Such affinities could provide important clues for accurate gallery image ranking but were only utilized in post-processing stages by current methods. In this article, we propose a unified end-to-end deep learning framework to tackle the two challenges. For handling viewpoint and pose variations between compared person images, we propose a novel Kronecker Product Matching operation to match and warp feature maps of different persons. Comparing warped feature maps results in more accurate P2G affinities. To fully utilize all available P2G and G2G affinities for accurately ranking gallery person images, a novel group-shuffling random walk operation is proposed. Both Kronecker Product Matching and Group-shuffling Random Walk operations are end-to-end trainable and are shown to improve the learned visual features if integrated in the deep learning framework. The proposed approach outperforms state-of-the-art methods on Market-1501, CUHK03 and DukeMTMC datasets, which demonstrates the effectiveness and generalization ability of our proposed approach. Code is available at https://github.com/YantaoShen/kpm_rw_person_reid. \n",
      "\n",
      "Crack detection in images of masonry using cnns While there is a significant body of research on crack detection by computer vision methods in concrete and asphalt, less attention has been given to masonry. We train a convolutional neural network (CNN) on images of brick walls built in a laboratory environment and test its ability to detect cracks in images of brick-and-mortar structures both in the laboratory and on real-world images taken from the internet. We also compare the performance of the CNN to a variety of simpler classifiers operating on handcrafted features. We find that the CNN performed better on the domain adaptation from laboratory to real-world images than these simple models. However, we also find that performance is significantly better in performing the reverse domain adaptation task, where the simple classifiers are trained on real-world images and tested on the laboratory images. This work demonstrates the ability to detect cracks in images of masonry using a variety of machine learning methods and provides guidance for improving the reliability of such models when performing domain adaptation for crack detection in masonry. \n",
      "\n",
      "Towards an energy efficient code generator for mobile phones Using a smartphone become the part of our everyday life in the last few years. These devices can help us in many areas of life (sport, job, weather etc.), but sometimes can be also very annoying because of the battery life time. That is why it is very important to find solutions, which can reduce the energy consumption of the smartphones. One possible method is the 'computation offloading' where a part of the processes are executed on a remote device (e.g. in the cloud). A lot of example has already shown, that computation offloading can reduce the energy usage of the mobile devices. However, the amount of energy saving may differ, as the decision making of the offloading process can be controlled with several techniques. Our decision making theory is based on the scheduling theory. In this paper, we are going to introduce a new system called ECGM (Energy efficient Code Generator for Mobile phones). ECGM decides automatically at compile time, which task should run on the smartphone and which task can be offloaded. The benefit of our system will be demonstrated through measurements based on our energy-efficiency scheduling technique. \n",
      "\n",
      "Sub-polyhedral scheduling using (Unit-)two-variable-per-inequality polyhedra Polyhedral compilation has been successful in the design and implementation of complex loop nest optimizers and parallelizing compilers. The algorithmic complexity and scalability limitations remain one important weakness. We address it using sub-polyhedral under-aproximations of the systems of constraints resulting from affine scheduling problems. We propose a sub-polyhedral scheduling technique using (Unit-)Two-Variable-Per-Inequality or (U)TVPI Polyhedra. This technique relies on simple polynomial time algorithms to under-approximate a general polyhedron into (U)TVPI polyhedra. We modify the state-of-the-art PLuTo compiler using our scheduling technique, and show that for a majority of the Polybench (2.0) kernels, the above under-approximations yield polyhedra that are non-empty. Solving the under-approximated system leads to asymptotic gains in complexity, and shows practically significant improvements when compared to a traditional LP solver. We also verify that code generated by our sub-polyhedral parallelization prototype matches the performance of PLuTo-optimized code when the under-approximation preserves feasibility. Copyright \n",
      "\n",
      "Extracting multiple viewpoint models from relational databases Much time in process mining projects is spent on finding and understanding data sources and extracting the event data needed. As a result, only a fraction of time is spent actually applying techniques to discover, control and predict the business process. Moreover, current process mining techniques assume a single case notion. However, in real-life processes often different case notions are intertwined. For example, events of the same order handling process may refer to customers, orders, order lines, deliveries, and payments. Therefore, we propose to use Multiple Viewpoint (MVP) models that relate events through objects and that relate activities through classes. The required event data are much closer to existing relational databases. MVP models provide a holistic view on the process, but also allow for the extraction of classical event logs using different viewpoints. This way existing process mining techniques can be used for each viewpoint without the need for new data extractions and transformations. We provide a toolchain allowing for the discovery of MVP models (annotated with performance and frequency information) from relational databases. Moreover, we demonstrate that classical process mining techniques can be applied to any selected viewpoint. \n",
      "\n",
      "A Program Result Checker for the Lexical Analysis of the GNU C Compiler In theory, program result checking has been established as a well-suited method to construct formally correct compiler frontends but it has never proved its practicality for real-life compilers. Such a proof is necessary to establish result checking as the method of choice to implement compilers correctly. We show that the lexical analysis of the GNU C compiler can be formally specified and checked within the theorem prover Isabelle/HOL utilizing program checking. Thereby we demonstrate that formal specification and verification techniques are able to handle real-life compilers. \n",
      "\n",
      "Advances in visual object tracking algorithm based on correlation filter  With excellent comprehensive performance, correlation filter-based tracking algorithms have become a hotspot of the theoretical research and practical application in the field of visual object tracking. Despite many studies, there is still a lack of systematic analyses on the existing correlation filter-based tracking algorithms from the level of tracking framework. Therefore, starting from the basic framework of object tracking algorithms, the characteristics of correlation filter-based tracking algorithms are deeply analyzed, and their basic problems in each working stage are presented in this paper. On this basis, the main technological progress of correlation filter-based tracking algorithms and characteristics of corresponding algorithms in recent ten years are summarized, and 20 typical correlation filter-based tracking algorithms are evaluated and analyzed. Finally, the outstanding issues to be urgently solved and the future research directions of correlation filter-based tracking algorithms are discussed. \n",
      "\n",
      "A Study on Various Database Models: Relational, Graph, and Hybrid Databases Relational database is a popular database for storing various types of information. But due to the ever-increasing growth of data, it becomes hard to maintain and process the database. So, the graph model is becoming more and more popular since it can store and handle big data more efficiently compared to relational database. But both relational database and graph database have their own advantages and disadvantages. To overcome their limitations, they are combined to make a hybrid model. This paper discusses relational database, graph database, their advantages, their applications and also talks about hybrid model. \n",
      "\n",
      "Better Reporting of Studies on Artificial Intelligence: CONSORT-AI and Beyond An increasing number of studies on artificial intelligence (AI) are published in the dental and oral sciences. The reporting, but also further aspects of these studies, suffer from a range of limitations. Standards towards reporting, like the recently published Consolidated Standards of Reporting Trials (CONSORT)-AI extension can help to improve studies in this emerging field, and the Journal of Dental Research (JDR) encourages authors, reviewers, and readers to adhere to these standards. Notably, though, a wide range of aspects beyond reporting, located along various steps of the AI lifecycle, should be considered when conceiving, conducting, reporting, or evaluating studies on AI in dentistry. \n"
     ]
    }
   ],
   "source": [
    "# Load raw data\n",
    "data_path = 'scopusabstracts.txt'\n",
    "\n",
    "reader = open(data_path, 'r', encoding='utf-8')\n",
    "lines = reader.readlines()\n",
    "\n",
    "# Extract the text (title + abstract) from each line\n",
    "titles = [i.split('#')[1] for i in lines[1:]]\n",
    "texts = [i.split('#')[2] for i in lines[1:]]\n",
    "\n",
    "# removing foreign language titles\n",
    "titles = [re.sub(r'\\[+.+]+$', '', title.strip()) for title in titles]\n",
    "\n",
    "# concatenating title to abstract\n",
    "texts = [titles[i] + ' ' + texts[i] for i in range(len(titles))]\n",
    "\n",
    "# uncomment this line and run the file again to get TF-IDF and clustering on just the titles\n",
    "# texts = titles  # un\n",
    "\n",
    "# some examples\n",
    "print('First documents:')\n",
    "for i in texts[:10]:\n",
    "    print(i)\n",
    "print()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-24T14:40:56.650619700Z",
     "start_time": "2023-11-24T14:40:56.480399900Z"
    }
   },
   "id": "ff04564e1d43a4f0"
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After tokenization and lowercasing:\n",
      "['anomaly', 'detection', 'in', 'wide', 'area', 'imagery', 'this', 'study', 'is', 'about', 'detecting', 'anomalies', 'in', 'wide', 'area', 'imagery', 'collected', 'from', 'an', 'aircraft', '.', 'the', 'set', 'of', 'anomalies', 'have', 'been', 'identified', 'as', 'anything', 'out', 'of', 'the', 'normal', 'course', 'of', 'action', '.', 'for', 'this', 'purpose', ',', 'two', 'different', 'data', 'sets', 'were', 'used', 'and', 'the', 'experiments', 'were', 'carried', 'out', 'on', 'these', 'data', 'sets', '.', 'for', 'anomaly', 'detection', ',', 'a', 'convolutional', 'neural', 'network', 'model', 'that', 'tries', 'to', 'generate', 'the', 'next', 'image', 'using', 'past', 'images', 'is', 'designed', '.', 'the', 'images', 'were', 'pre-processed', 'before', 'being', 'given', 'to', 'the', 'model', '.', 'anomaly', 'detection', 'is', 'performed', 'by', 'comparing', 'the', 'estimated', 'image', 'and', 'the', 'true', 'image', '.']\n",
      "['person', 're-identification', 'with', 'deep', 'kronecker-product', 'matching', 'and', 'group-shuffling', 'random', 'walk', 'person', 're-identification', '(', 're-id', ')', 'aims', 'to', 'robustly', 'measure', 'visual', 'affinities', 'between', 'person', 'images', '.', 'it', 'has', 'wide', 'applications', 'in', 'intelligent', 'surveillance', 'by', 'associating', 'same', 'persons', \"'\", 'images', 'across', 'multiple', 'cameras', '.', 'it', 'is', 'generally', 'treated', 'as', 'an', 'image', 'retrieval', 'problem', ':', 'given', 'a', 'probe', 'person', 'image', ',', 'the', 'affinities', 'between', 'the', 'probe', 'image', 'and', 'gallery', 'images', '(', 'p2g', 'affinities', ')', 'are', 'used', 'to', 'rank', 'the', 'retrieved', 'gallery', 'images', '.', 'there', 'exist', 'two', 'main', 'challenges', 'for', 'effectively', 'solving', 'this', 'problem', '.', '1', ')', 'person', 'images', 'usually', 'show', 'significant', 'variations', 'because', 'of', 'different', 'person', 'poses', 'and', 'viewing', 'angles', '.', 'the', 'spatial', 'layouts', 'and', 'correspondences', 'between', 'person', 'images', 'are', 'therefore', 'vital', 'information', 'for', 'tackling', 'this', 'problem', '.', 'state-of-the-art', 'methods', 'either', 'ignore', 'such', 'spatial', 'variation', 'or', 'utilize', 'extra', 'pose', 'information', 'for', 'handling', 'the', 'challenge', '.', '2', ')', 'most', 'existing', 'person', 're-id', 'methods', 'rank', 'gallery', 'images', 'considering', 'only', 'p2g', 'affinities', 'but', 'ignore', 'the', 'affinities', 'between', 'the', 'gallery', 'images', '(', 'g2g', 'affinity', ')', '.', 'such', 'affinities', 'could', 'provide', 'important', 'clues', 'for', 'accurate', 'gallery', 'image', 'ranking', 'but', 'were', 'only', 'utilized', 'in', 'post-processing', 'stages', 'by', 'current', 'methods', '.', 'in', 'this', 'article', ',', 'we', 'propose', 'a', 'unified', 'end-to-end', 'deep', 'learning', 'framework', 'to', 'tackle', 'the', 'two', 'challenges', '.', 'for', 'handling', 'viewpoint', 'and', 'pose', 'variations', 'between', 'compared', 'person', 'images', ',', 'we', 'propose', 'a', 'novel', 'kronecker', 'product', 'matching', 'operation', 'to', 'match', 'and', 'warp', 'feature', 'maps', 'of', 'different', 'persons', '.', 'comparing', 'warped', 'feature', 'maps', 'results', 'in', 'more', 'accurate', 'p2g', 'affinities', '.', 'to', 'fully', 'utilize', 'all', 'available', 'p2g', 'and', 'g2g', 'affinities', 'for', 'accurately', 'ranking', 'gallery', 'person', 'images', ',', 'a', 'novel', 'group-shuffling', 'random', 'walk', 'operation', 'is', 'proposed', '.', 'both', 'kronecker', 'product', 'matching', 'and', 'group-shuffling', 'random', 'walk', 'operations', 'are', 'end-to-end', 'trainable', 'and', 'are', 'shown', 'to', 'improve', 'the', 'learned', 'visual', 'features', 'if', 'integrated', 'in', 'the', 'deep', 'learning', 'framework', '.', 'the', 'proposed', 'approach', 'outperforms', 'state-of-the-art', 'methods', 'on', 'market-1501', ',', 'cuhk03', 'and', 'dukemtmc', 'datasets', ',', 'which', 'demonstrates', 'the', 'effectiveness', 'and', 'generalization', 'ability', 'of', 'our', 'proposed', 'approach', '.', 'code', 'is', 'available', 'at', 'https', ':', '//github.com/yantaoshen/kpm_rw_person_reid', '.']\n",
      "['crack', 'detection', 'in', 'images', 'of', 'masonry', 'using', 'cnns', 'while', 'there', 'is', 'a', 'significant', 'body', 'of', 'research', 'on', 'crack', 'detection', 'by', 'computer', 'vision', 'methods', 'in', 'concrete', 'and', 'asphalt', ',', 'less', 'attention', 'has', 'been', 'given', 'to', 'masonry', '.', 'we', 'train', 'a', 'convolutional', 'neural', 'network', '(', 'cnn', ')', 'on', 'images', 'of', 'brick', 'walls', 'built', 'in', 'a', 'laboratory', 'environment', 'and', 'test', 'its', 'ability', 'to', 'detect', 'cracks', 'in', 'images', 'of', 'brick-and-mortar', 'structures', 'both', 'in', 'the', 'laboratory', 'and', 'on', 'real-world', 'images', 'taken', 'from', 'the', 'internet', '.', 'we', 'also', 'compare', 'the', 'performance', 'of', 'the', 'cnn', 'to', 'a', 'variety', 'of', 'simpler', 'classifiers', 'operating', 'on', 'handcrafted', 'features', '.', 'we', 'find', 'that', 'the', 'cnn', 'performed', 'better', 'on', 'the', 'domain', 'adaptation', 'from', 'laboratory', 'to', 'real-world', 'images', 'than', 'these', 'simple', 'models', '.', 'however', ',', 'we', 'also', 'find', 'that', 'performance', 'is', 'significantly', 'better', 'in', 'performing', 'the', 'reverse', 'domain', 'adaptation', 'task', ',', 'where', 'the', 'simple', 'classifiers', 'are', 'trained', 'on', 'real-world', 'images', 'and', 'tested', 'on', 'the', 'laboratory', 'images', '.', 'this', 'work', 'demonstrates', 'the', 'ability', 'to', 'detect', 'cracks', 'in', 'images', 'of', 'masonry', 'using', 'a', 'variety', 'of', 'machine', 'learning', 'methods', 'and', 'provides', 'guidance', 'for', 'improving', 'the', 'reliability', 'of', 'such', 'models', 'when', 'performing', 'domain', 'adaptation', 'for', 'crack', 'detection', 'in', 'masonry', '.']\n",
      "['towards', 'an', 'energy', 'efficient', 'code', 'generator', 'for', 'mobile', 'phones', 'using', 'a', 'smartphone', 'become', 'the', 'part', 'of', 'our', 'everyday', 'life', 'in', 'the', 'last', 'few', 'years', '.', 'these', 'devices', 'can', 'help', 'us', 'in', 'many', 'areas', 'of', 'life', '(', 'sport', ',', 'job', ',', 'weather', 'etc', '.', ')', ',', 'but', 'sometimes', 'can', 'be', 'also', 'very', 'annoying', 'because', 'of', 'the', 'battery', 'life', 'time', '.', 'that', 'is', 'why', 'it', 'is', 'very', 'important', 'to', 'find', 'solutions', ',', 'which', 'can', 'reduce', 'the', 'energy', 'consumption', 'of', 'the', 'smartphones', '.', 'one', 'possible', 'method', 'is', 'the', \"'computation\", 'offloading', \"'\", 'where', 'a', 'part', 'of', 'the', 'processes', 'are', 'executed', 'on', 'a', 'remote', 'device', '(', 'e.g', '.', 'in', 'the', 'cloud', ')', '.', 'a', 'lot', 'of', 'example', 'has', 'already', 'shown', ',', 'that', 'computation', 'offloading', 'can', 'reduce', 'the', 'energy', 'usage', 'of', 'the', 'mobile', 'devices', '.', 'however', ',', 'the', 'amount', 'of', 'energy', 'saving', 'may', 'differ', ',', 'as', 'the', 'decision', 'making', 'of', 'the', 'offloading', 'process', 'can', 'be', 'controlled', 'with', 'several', 'techniques', '.', 'our', 'decision', 'making', 'theory', 'is', 'based', 'on', 'the', 'scheduling', 'theory', '.', 'in', 'this', 'paper', ',', 'we', 'are', 'going', 'to', 'introduce', 'a', 'new', 'system', 'called', 'ecgm', '(', 'energy', 'efficient', 'code', 'generator', 'for', 'mobile', 'phones', ')', '.', 'ecgm', 'decides', 'automatically', 'at', 'compile', 'time', ',', 'which', 'task', 'should', 'run', 'on', 'the', 'smartphone', 'and', 'which', 'task', 'can', 'be', 'offloaded', '.', 'the', 'benefit', 'of', 'our', 'system', 'will', 'be', 'demonstrated', 'through', 'measurements', 'based', 'on', 'our', 'energy-efficiency', 'scheduling', 'technique', '.']\n",
      "['sub-polyhedral', 'scheduling', 'using', '(', 'unit-', ')', 'two-variable-per-inequality', 'polyhedra', 'polyhedral', 'compilation', 'has', 'been', 'successful', 'in', 'the', 'design', 'and', 'implementation', 'of', 'complex', 'loop', 'nest', 'optimizers', 'and', 'parallelizing', 'compilers', '.', 'the', 'algorithmic', 'complexity', 'and', 'scalability', 'limitations', 'remain', 'one', 'important', 'weakness', '.', 'we', 'address', 'it', 'using', 'sub-polyhedral', 'under-aproximations', 'of', 'the', 'systems', 'of', 'constraints', 'resulting', 'from', 'affine', 'scheduling', 'problems', '.', 'we', 'propose', 'a', 'sub-polyhedral', 'scheduling', 'technique', 'using', '(', 'unit-', ')', 'two-variable-per-inequality', 'or', '(', 'u', ')', 'tvpi', 'polyhedra', '.', 'this', 'technique', 'relies', 'on', 'simple', 'polynomial', 'time', 'algorithms', 'to', 'under-approximate', 'a', 'general', 'polyhedron', 'into', '(', 'u', ')', 'tvpi', 'polyhedra', '.', 'we', 'modify', 'the', 'state-of-the-art', 'pluto', 'compiler', 'using', 'our', 'scheduling', 'technique', ',', 'and', 'show', 'that', 'for', 'a', 'majority', 'of', 'the', 'polybench', '(', '2.0', ')', 'kernels', ',', 'the', 'above', 'under-approximations', 'yield', 'polyhedra', 'that', 'are', 'non-empty', '.', 'solving', 'the', 'under-approximated', 'system', 'leads', 'to', 'asymptotic', 'gains', 'in', 'complexity', ',', 'and', 'shows', 'practically', 'significant', 'improvements', 'when', 'compared', 'to', 'a', 'traditional', 'lp', 'solver', '.', 'we', 'also', 'verify', 'that', 'code', 'generated', 'by', 'our', 'sub-polyhedral', 'parallelization', 'prototype', 'matches', 'the', 'performance', 'of', 'pluto-optimized', 'code', 'when', 'the', 'under-approximation', 'preserves', 'feasibility', '.', 'copyright']\n",
      "['extracting', 'multiple', 'viewpoint', 'models', 'from', 'relational', 'databases', 'much', 'time', 'in', 'process', 'mining', 'projects', 'is', 'spent', 'on', 'finding', 'and', 'understanding', 'data', 'sources', 'and', 'extracting', 'the', 'event', 'data', 'needed', '.', 'as', 'a', 'result', ',', 'only', 'a', 'fraction', 'of', 'time', 'is', 'spent', 'actually', 'applying', 'techniques', 'to', 'discover', ',', 'control', 'and', 'predict', 'the', 'business', 'process', '.', 'moreover', ',', 'current', 'process', 'mining', 'techniques', 'assume', 'a', 'single', 'case', 'notion', '.', 'however', ',', 'in', 'real-life', 'processes', 'often', 'different', 'case', 'notions', 'are', 'intertwined', '.', 'for', 'example', ',', 'events', 'of', 'the', 'same', 'order', 'handling', 'process', 'may', 'refer', 'to', 'customers', ',', 'orders', ',', 'order', 'lines', ',', 'deliveries', ',', 'and', 'payments', '.', 'therefore', ',', 'we', 'propose', 'to', 'use', 'multiple', 'viewpoint', '(', 'mvp', ')', 'models', 'that', 'relate', 'events', 'through', 'objects', 'and', 'that', 'relate', 'activities', 'through', 'classes', '.', 'the', 'required', 'event', 'data', 'are', 'much', 'closer', 'to', 'existing', 'relational', 'databases', '.', 'mvp', 'models', 'provide', 'a', 'holistic', 'view', 'on', 'the', 'process', ',', 'but', 'also', 'allow', 'for', 'the', 'extraction', 'of', 'classical', 'event', 'logs', 'using', 'different', 'viewpoints', '.', 'this', 'way', 'existing', 'process', 'mining', 'techniques', 'can', 'be', 'used', 'for', 'each', 'viewpoint', 'without', 'the', 'need', 'for', 'new', 'data', 'extractions', 'and', 'transformations', '.', 'we', 'provide', 'a', 'toolchain', 'allowing', 'for', 'the', 'discovery', 'of', 'mvp', 'models', '(', 'annotated', 'with', 'performance', 'and', 'frequency', 'information', ')', 'from', 'relational', 'databases', '.', 'moreover', ',', 'we', 'demonstrate', 'that', 'classical', 'process', 'mining', 'techniques', 'can', 'be', 'applied', 'to', 'any', 'selected', 'viewpoint', '.']\n",
      "['a', 'program', 'result', 'checker', 'for', 'the', 'lexical', 'analysis', 'of', 'the', 'gnu', 'c', 'compiler', 'in', 'theory', ',', 'program', 'result', 'checking', 'has', 'been', 'established', 'as', 'a', 'well-suited', 'method', 'to', 'construct', 'formally', 'correct', 'compiler', 'frontends', 'but', 'it', 'has', 'never', 'proved', 'its', 'practicality', 'for', 'real-life', 'compilers', '.', 'such', 'a', 'proof', 'is', 'necessary', 'to', 'establish', 'result', 'checking', 'as', 'the', 'method', 'of', 'choice', 'to', 'implement', 'compilers', 'correctly', '.', 'we', 'show', 'that', 'the', 'lexical', 'analysis', 'of', 'the', 'gnu', 'c', 'compiler', 'can', 'be', 'formally', 'specified', 'and', 'checked', 'within', 'the', 'theorem', 'prover', 'isabelle/hol', 'utilizing', 'program', 'checking', '.', 'thereby', 'we', 'demonstrate', 'that', 'formal', 'specification', 'and', 'verification', 'techniques', 'are', 'able', 'to', 'handle', 'real-life', 'compilers', '.']\n",
      "['advances', 'in', 'visual', 'object', 'tracking', 'algorithm', 'based', 'on', 'correlation', 'filter', 'with', 'excellent', 'comprehensive', 'performance', ',', 'correlation', 'filter-based', 'tracking', 'algorithms', 'have', 'become', 'a', 'hotspot', 'of', 'the', 'theoretical', 'research', 'and', 'practical', 'application', 'in', 'the', 'field', 'of', 'visual', 'object', 'tracking', '.', 'despite', 'many', 'studies', ',', 'there', 'is', 'still', 'a', 'lack', 'of', 'systematic', 'analyses', 'on', 'the', 'existing', 'correlation', 'filter-based', 'tracking', 'algorithms', 'from', 'the', 'level', 'of', 'tracking', 'framework', '.', 'therefore', ',', 'starting', 'from', 'the', 'basic', 'framework', 'of', 'object', 'tracking', 'algorithms', ',', 'the', 'characteristics', 'of', 'correlation', 'filter-based', 'tracking', 'algorithms', 'are', 'deeply', 'analyzed', ',', 'and', 'their', 'basic', 'problems', 'in', 'each', 'working', 'stage', 'are', 'presented', 'in', 'this', 'paper', '.', 'on', 'this', 'basis', ',', 'the', 'main', 'technological', 'progress', 'of', 'correlation', 'filter-based', 'tracking', 'algorithms', 'and', 'characteristics', 'of', 'corresponding', 'algorithms', 'in', 'recent', 'ten', 'years', 'are', 'summarized', ',', 'and', '20', 'typical', 'correlation', 'filter-based', 'tracking', 'algorithms', 'are', 'evaluated', 'and', 'analyzed', '.', 'finally', ',', 'the', 'outstanding', 'issues', 'to', 'be', 'urgently', 'solved', 'and', 'the', 'future', 'research', 'directions', 'of', 'correlation', 'filter-based', 'tracking', 'algorithms', 'are', 'discussed', '.']\n",
      "['a', 'study', 'on', 'various', 'database', 'models', ':', 'relational', ',', 'graph', ',', 'and', 'hybrid', 'databases', 'relational', 'database', 'is', 'a', 'popular', 'database', 'for', 'storing', 'various', 'types', 'of', 'information', '.', 'but', 'due', 'to', 'the', 'ever-increasing', 'growth', 'of', 'data', ',', 'it', 'becomes', 'hard', 'to', 'maintain', 'and', 'process', 'the', 'database', '.', 'so', ',', 'the', 'graph', 'model', 'is', 'becoming', 'more', 'and', 'more', 'popular', 'since', 'it', 'can', 'store', 'and', 'handle', 'big', 'data', 'more', 'efficiently', 'compared', 'to', 'relational', 'database', '.', 'but', 'both', 'relational', 'database', 'and', 'graph', 'database', 'have', 'their', 'own', 'advantages', 'and', 'disadvantages', '.', 'to', 'overcome', 'their', 'limitations', ',', 'they', 'are', 'combined', 'to', 'make', 'a', 'hybrid', 'model', '.', 'this', 'paper', 'discusses', 'relational', 'database', ',', 'graph', 'database', ',', 'their', 'advantages', ',', 'their', 'applications', 'and', 'also', 'talks', 'about', 'hybrid', 'model', '.']\n",
      "['better', 'reporting', 'of', 'studies', 'on', 'artificial', 'intelligence', ':', 'consort-ai', 'and', 'beyond', 'an', 'increasing', 'number', 'of', 'studies', 'on', 'artificial', 'intelligence', '(', 'ai', ')', 'are', 'published', 'in', 'the', 'dental', 'and', 'oral', 'sciences', '.', 'the', 'reporting', ',', 'but', 'also', 'further', 'aspects', 'of', 'these', 'studies', ',', 'suffer', 'from', 'a', 'range', 'of', 'limitations', '.', 'standards', 'towards', 'reporting', ',', 'like', 'the', 'recently', 'published', 'consolidated', 'standards', 'of', 'reporting', 'trials', '(', 'consort', ')', '-ai', 'extension', 'can', 'help', 'to', 'improve', 'studies', 'in', 'this', 'emerging', 'field', ',', 'and', 'the', 'journal', 'of', 'dental', 'research', '(', 'jdr', ')', 'encourages', 'authors', ',', 'reviewers', ',', 'and', 'readers', 'to', 'adhere', 'to', 'these', 'standards', '.', 'notably', ',', 'though', ',', 'a', 'wide', 'range', 'of', 'aspects', 'beyond', 'reporting', ',', 'located', 'along', 'various', 'steps', 'of', 'the', 'ai', 'lifecycle', ',', 'should', 'be', 'considered', 'when', 'conceiving', ',', 'conducting', ',', 'reporting', ',', 'or', 'evaluating', 'studies', 'on', 'ai', 'in', 'dentistry', '.']\n",
      "\n",
      "Original number of tokens: 15813\n",
      "\n",
      "NLTK stopwords:\n",
      "{'once', 'am', 'can', 'most', \"it's\", 'where', 'yourselves', 've', 't', 'i', 'don', 'our', 'themselves', 'll', 'out', 'down', 'weren', 'own', 'have', 'for', 'theirs', 'there', 'an', 'and', 'ma', 'is', \"hasn't\", 'the', 'what', 'ourselves', \"wasn't\", 'mightn', \"weren't\", 'were', 'any', 'ours', 'only', 'himself', 'not', 'same', 'itself', 'yours', 'she', 'are', 'doesn', 'hasn', 'shan', \"she's\", 's', \"hadn't\", 'between', 'his', 'few', 'will', \"don't\", 'mustn', 'didn', 'isn', 'doing', 'but', 'above', 'more', \"couldn't\", 'under', 'hadn', 'do', \"you're\", 'd', 'until', \"aren't\", 'too', 'whom', \"didn't\", 'against', 'those', 'yourself', 'through', 'from', 'why', 'when', 'being', 'these', \"doesn't\", \"isn't\", 'couldn', 'had', 'each', 'aren', 'by', 'now', 'into', \"haven't\", 'hers', 'in', 'you', 'be', 'so', 'won', 'does', 'your', 'while', 'haven', 'again', 'other', 'me', 'been', 'below', 'very', 'its', \"you've\", 'we', 'm', 'myself', 'such', 'they', 'over', 'o', 'y', \"shouldn't\", 're', 'here', 'about', 'as', 'who', 'because', \"won't\", 'my', 'this', 'with', \"should've\", 'all', 'was', 'their', 'just', 'further', 'that', 'at', 'it', 'did', \"mightn't\", \"you'll\", 'on', 'needn', 'during', 'herself', 'her', 'wasn', 'them', 'or', 'some', 'up', 'him', 'before', 'of', 'after', 'wouldn', 'ain', 'shouldn', \"that'll\", 'off', 'which', \"wouldn't\", 'he', \"you'd\", 'has', 'having', \"needn't\", 'a', \"shan't\", 'than', 'how', 'both', 'should', 'if', 'to', \"mustn't\", 'nor', 'no', 'then'}\n",
      "\n",
      "Number of tokens after stopword and punctuation removal: 14751\n",
      "\n",
      "After removing stop words, punctuation and numbers:\n",
      "anomaly detection wide area imagery study detecting anomalies wide area imagery collected aircraft set anomalies identified anything normal course action purpose two different data sets used experiments carried data sets anomaly detection convolutional neural network model tries generate next image using past images designed images pre-processed given model anomaly detection performed comparing estimated image true image\n",
      "person re-identification deep kronecker-product matching group-shuffling random walk person re-identification re-id aims robustly measure visual affinities person images wide applications intelligent surveillance associating persons images across multiple cameras generally treated image retrieval problem given probe person image affinities probe image gallery images pg affinities used rank retrieved gallery images exist two main challenges effectively solving problem  person images usually show significant variations different person poses viewing angles spatial layouts correspondences person images therefore vital information tackling problem state-of-the-art methods either ignore spatial variation utilize extra pose information handling challenge  existing person re-id methods rank gallery images considering pg affinities ignore affinities gallery images gg affinity affinities could provide important clues accurate gallery image ranking utilized post-processing stages current methods article propose unified end-to-end deep learning framework tackle two challenges handling viewpoint pose variations compared person images propose novel kronecker product matching operation match warp feature maps different persons comparing warped feature maps results accurate pg affinities fully utilize available pg gg affinities accurately ranking gallery person images novel group-shuffling random walk operation proposed kronecker product matching group-shuffling random walk operations end-to-end trainable shown improve learned visual features integrated deep learning framework proposed approach outperforms state-of-the-art methods market- cuhk dukemtmc datasets demonstrates effectiveness generalization ability proposed approach code available https //github.com/yantaoshen/kpm_rw_person_reid\n",
      "crack detection images masonry using cnns significant body research crack detection computer vision methods concrete asphalt less attention given masonry train convolutional neural network cnn images brick walls built laboratory environment test ability detect cracks images brick-and-mortar structures laboratory real-world images taken internet also compare performance cnn variety simpler classifiers operating handcrafted features find cnn performed better domain adaptation laboratory real-world images simple models however also find performance significantly better performing reverse domain adaptation task simple classifiers trained real-world images tested laboratory images work demonstrates ability detect cracks images masonry using variety machine learning methods provides guidance improving reliability models performing domain adaptation crack detection masonry\n",
      "towards energy efficient code generator mobile phones using smartphone become part everyday life last years devices help us many areas life sport job weather etc sometimes also annoying battery life time important find solutions reduce energy consumption smartphones one possible method 'computation offloading part processes executed remote device e.g cloud lot example already shown computation offloading reduce energy usage mobile devices however amount energy saving may differ decision making offloading process controlled several techniques decision making theory based scheduling theory paper going introduce new system called ecgm energy efficient code generator mobile phones ecgm decides automatically compile time task run smartphone task offloaded benefit system demonstrated measurements based energy-efficiency scheduling technique\n",
      "sub-polyhedral scheduling using unit- two-variable-per-inequality polyhedra polyhedral compilation successful design implementation complex loop nest optimizers parallelizing compilers algorithmic complexity scalability limitations remain one important weakness address using sub-polyhedral under-aproximations systems constraints resulting affine scheduling problems propose sub-polyhedral scheduling technique using unit- two-variable-per-inequality u tvpi polyhedra technique relies simple polynomial time algorithms under-approximate general polyhedron u tvpi polyhedra modify state-of-the-art pluto compiler using scheduling technique show majority polybench . kernels under-approximations yield polyhedra non-empty solving under-approximated system leads asymptotic gains complexity shows practically significant improvements compared traditional lp solver also verify code generated sub-polyhedral parallelization prototype matches performance pluto-optimized code under-approximation preserves feasibility copyright\n",
      "extracting multiple viewpoint models relational databases much time process mining projects spent finding understanding data sources extracting event data needed result fraction time spent actually applying techniques discover control predict business process moreover current process mining techniques assume single case notion however real-life processes often different case notions intertwined example events order handling process may refer customers orders order lines deliveries payments therefore propose use multiple viewpoint mvp models relate events objects relate activities classes required event data much closer existing relational databases mvp models provide holistic view process also allow extraction classical event logs using different viewpoints way existing process mining techniques used viewpoint without need new data extractions transformations provide toolchain allowing discovery mvp models annotated performance frequency information relational databases moreover demonstrate classical process mining techniques applied selected viewpoint\n",
      "program result checker lexical analysis gnu c compiler theory program result checking established well-suited method construct formally correct compiler frontends never proved practicality real-life compilers proof necessary establish result checking method choice implement compilers correctly show lexical analysis gnu c compiler formally specified checked within theorem prover isabelle/hol utilizing program checking thereby demonstrate formal specification verification techniques able handle real-life compilers\n",
      "advances visual object tracking algorithm based correlation filter excellent comprehensive performance correlation filter-based tracking algorithms become hotspot theoretical research practical application field visual object tracking despite many studies still lack systematic analyses existing correlation filter-based tracking algorithms level tracking framework therefore starting basic framework object tracking algorithms characteristics correlation filter-based tracking algorithms deeply analyzed basic problems working stage presented paper basis main technological progress correlation filter-based tracking algorithms characteristics corresponding algorithms recent ten years summarized  typical correlation filter-based tracking algorithms evaluated analyzed finally outstanding issues urgently solved future research directions correlation filter-based tracking algorithms discussed\n",
      "study various database models relational graph hybrid databases relational database popular database storing various types information due ever-increasing growth data becomes hard maintain process database graph model becoming popular since store handle big data efficiently compared relational database relational database graph database advantages disadvantages overcome limitations combined make hybrid model paper discusses relational database graph database advantages applications also talks hybrid model\n",
      "better reporting studies artificial intelligence consort-ai beyond increasing number studies artificial intelligence ai published dental oral sciences reporting also aspects studies suffer range limitations standards towards reporting like recently published consolidated standards reporting trials consort -ai extension help improve studies emerging field journal dental research jdr encourages authors reviewers readers adhere standards notably though wide range aspects beyond reporting located along various steps ai lifecycle considered conceiving conducting reporting evaluating studies ai dentistry\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing\n",
    "\n",
    "# Step 1: tokenization and lowercasing\n",
    "tokens_list = [word_tokenize(i) for i in texts]\n",
    "\n",
    "lc_tokens_list = []\n",
    "for i in tokens_list:\n",
    "    lc_tokens_list.append([token.lower() for token in i])\n",
    "\n",
    "print('After tokenization and lowercasing:')\n",
    "for i in lc_tokens_list[:10]:\n",
    "    print(i)\n",
    "print()\n",
    "\n",
    "# original number of tokens\n",
    "uniques = np.unique([tok for doc in lc_tokens_list for tok in doc])\n",
    "print(\"Original number of tokens: {}\\n\".format(len(uniques)))\n",
    "\n",
    "# Steps 2 and 3: remove stop words and punctuation\n",
    "stop_words = set(stopwords.words('english'))\n",
    "print('NLTK stopwords:')\n",
    "print(stop_words)\n",
    "print()\n",
    "\n",
    "# Here we include the punctuation in the stop words set. There are alternative\n",
    "# ways to remove punctuation.\n",
    "stop_words.update(string.punctuation)\n",
    "stop_words.add(\"...\")\n",
    "\n",
    "# you can check updated stopwords\n",
    "# print(stop_words)\n",
    "\n",
    "filtered_sentence = []\n",
    "for i in lc_tokens_list:\n",
    "    filtered_sentence.append([token for token in i if token not in stop_words])\n",
    "\n",
    "# Numbers are also removed\n",
    "filtered_sentence = [' '.join(i) for i in filtered_sentence]\n",
    "filtered_sentence = [re.sub(r'\\d+', '', sentence) for sentence in filtered_sentence]\n",
    "\n",
    "# number of tokens\n",
    "uniques = np.unique([tok for doc in filtered_sentence for tok in doc.split()])\n",
    "print(\"Number of tokens after stopword and punctuation removal: {}\\n\".format(len(uniques)))\n",
    "\n",
    "print('After removing stop words, punctuation and numbers:')\n",
    "for i in filtered_sentence[:10]:\n",
    "    print(i)\n",
    "print()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-24T14:40:58.792079800Z",
     "start_time": "2023-11-24T14:40:56.662056600Z"
    }
   },
   "id": "1242c9cdcd79ac87"
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens after stemming: 10191\n",
      "\n",
      "After stemming:\n",
      "anomali detect wide area imageri studi detect anomali wide area imageri collect aircraft set anomali identifi anyth normal cours action purpos two differ data set use experi carri data set anomali detect convolut neural network model tri gener next imag use past imag design imag pre-process given model anomali detect perform compar estim imag true imag  \n",
      "person re-identif deep kronecker-product match group-shuffl random walk person re-identif re-id aim robustli measur visual affin person imag wide applic intellig surveil associ person imag across multipl camera gener treat imag retriev problem given probe person imag affin probe imag galleri imag pg affin use rank retriev galleri imag exist two main challeng effect solv problem person imag usual show signific variat differ person pose view angl spatial layout correspond person imag therefor vital inform tackl problem state-of-the-art method either ignor spatial variat util extra pose inform handl challeng exist person re-id method rank galleri imag consid pg affin ignor affin galleri imag gg affin affin could provid import clue accur galleri imag rank util post-process stage current method articl propos unifi end-to-end deep learn framework tackl two challeng handl viewpoint pose variat compar person imag propos novel kroneck product match oper match warp featur map differ person compar warp featur map result accur pg affin fulli util avail pg gg affin accur rank galleri person imag novel group-shuffl random walk oper propos kroneck product match group-shuffl random walk oper end-to-end trainabl shown improv learn visual featur integr deep learn framework propos approach outperform state-of-the-art method market- cuhk dukemtmc dataset demonstr effect gener abil propos approach code avail http //github.com/yantaoshen/kpm_rw_person_reid  \n",
      "crack detect imag masonri use cnn signific bodi research crack detect comput vision method concret asphalt less attent given masonri train convolut neural network cnn imag brick wall built laboratori environ test abil detect crack imag brick-and-mortar structur laboratori real-world imag taken internet also compar perform cnn varieti simpler classifi oper handcraft featur find cnn perform better domain adapt laboratori real-world imag simpl model howev also find perform significantli better perform revers domain adapt task simpl classifi train real-world imag test laboratori imag work demonstr abil detect crack imag masonri use varieti machin learn method provid guidanc improv reliabl model perform domain adapt crack detect masonri  \n",
      "toward energi effici code gener mobil phone use smartphon becom part everyday life last year devic help us mani area life sport job weather etc sometim also annoy batteri life time import find solut reduc energi consumpt smartphon one possibl method 'comput offload part process execut remot devic e.g cloud lot exampl alreadi shown comput offload reduc energi usag mobil devic howev amount energi save may differ decis make offload process control sever techniqu decis make theori base schedul theori paper go introduc new system call ecgm energi effici code gener mobil phone ecgm decid automat compil time task run smartphon task offload benefit system demonstr measur base energy-effici schedul techniqu  \n",
      "sub-polyhedr schedul use unit- two-variable-per-inequ polyhedra polyhedr compil success design implement complex loop nest optim parallel compil algorithm complex scalabl limit remain one import weak address use sub-polyhedr under-aproxim system constraint result affin schedul problem propos sub-polyhedr schedul techniqu use unit- two-variable-per-inequ u tvpi polyhedra techniqu reli simpl polynomi time algorithm under-approxim gener polyhedron u tvpi polyhedra modifi state-of-the-art pluto compil use schedul techniqu show major polybench . kernel under-approxim yield polyhedra non-empti solv under-approxim system lead asymptot gain complex show practic signific improv compar tradit lp solver also verifi code gener sub-polyhedr parallel prototyp match perform pluto-optim code under-approxim preserv feasibl copyright  \n",
      "extract multipl viewpoint model relat databas much time process mine project spent find understand data sourc extract event data need result fraction time spent actual appli techniqu discov control predict busi process moreov current process mine techniqu assum singl case notion howev real-lif process often differ case notion intertwin exampl event order handl process may refer custom order order line deliveri payment therefor propos use multipl viewpoint mvp model relat event object relat activ class requir event data much closer exist relat databas mvp model provid holist view process also allow extract classic event log use differ viewpoint way exist process mine techniqu use viewpoint without need new data extract transform provid toolchain allow discoveri mvp model annot perform frequenc inform relat databas moreov demonstr classic process mine techniqu appli select viewpoint  \n",
      "program result checker lexic analysi gnu c compil theori program result check establish well-suit method construct formal correct compil frontend never prove practic real-lif compil proof necessari establish result check method choic implement compil correctli show lexic analysi gnu c compil formal specifi check within theorem prover isabelle/hol util program check therebi demonstr formal specif verif techniqu abl handl real-lif compil  \n",
      "advanc visual object track algorithm base correl filter excel comprehens perform correl filter-bas track algorithm becom hotspot theoret research practic applic field visual object track despit mani studi still lack systemat analys exist correl filter-bas track algorithm level track framework therefor start basic framework object track algorithm characterist correl filter-bas track algorithm deepli analyz basic problem work stage present paper basi main technolog progress correl filter-bas track algorithm characterist correspond algorithm recent ten year summar typic correl filter-bas track algorithm evalu analyz final outstand issu urgent solv futur research direct correl filter-bas track algorithm discuss  \n",
      "studi variou databas model relat graph hybrid databas relat databas popular databas store variou type inform due ever-increas growth data becom hard maintain process databas graph model becom popular sinc store handl big data effici compar relat databas relat databas graph databas advantag disadvantag overcom limit combin make hybrid model paper discuss relat databas graph databas advantag applic also talk hybrid model  \n",
      "better report studi artifici intellig consort-ai beyond increas number studi artifici intellig ai publish dental oral scienc report also aspect studi suffer rang limit standard toward report like recent publish consolid standard report trial consort -ai extens help improv studi emerg field journal dental research jdr encourag author review reader adher standard notabl though wide rang aspect beyond report locat along variou step ai lifecycl consid conceiv conduct report evalu studi ai dentistri  \n"
     ]
    }
   ],
   "source": [
    "# Step 4: stemming\n",
    "porter = PorterStemmer()\n",
    "# or snowball stemmer\n",
    "# stemmer = SnowballStemmer(\"english\",ignore_stopwords=True)\n",
    "stemmed_tokens_list = []\n",
    "\n",
    "for i in filtered_sentence:\n",
    "    stemmed_tokens_list.append([porter.stem(j) for j in i.split()])\n",
    "\n",
    "# number of tokens\n",
    "uniques = np.unique([tok for doc in stemmed_tokens_list for tok in doc])\n",
    "print(\"Number of tokens after stemming: {}\\n\".format(len(uniques)))\n",
    "\n",
    "print('After stemming:')\n",
    "for i in stemmed_tokens_list[:10]:\n",
    "    for j in i:\n",
    "        print(j, end=\" \")\n",
    "    print(\" \")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-24T14:41:01.191195400Z",
     "start_time": "2023-11-24T14:40:58.600581400Z"
    }
   },
   "id": "c2ff783bc4a66a72"
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Most common words (total 10191)\n",
      "[('use', 1793), ('data', 1238), ('system', 1208), ('propos', 1082), ('model', 937), ('method', 880), ('comput', 868), ('robot', 806), ('imag', 792), ('perform', 774), ('base', 728), ('algorithm', 719), ('databas', 701), ('result', 684), ('secur', 665), ('paper', 635), ('approach', 621), ('compil', 602), ('applic', 593), ('design', 569), ('gener', 548), ('learn', 543), ('develop', 535), ('detect', 513), ('process', 512), ('.', 512), ('inform', 507), ('network', 505), ('present', 504), ('implement', 481), ('differ', 470), ('improv', 445), ('relat', 445), ('provid', 441), ('show', 437), ('optim', 437), ('techniqu', 430), ('time', 428), ('studi', 417), ('program', 417), ('evalu', 386), ('also', 385), ('effici', 380), ('work', 374), ('problem', 366), ('analysi', 365), ('object', 361), ('scheme', 358), ('research', 348), ('new', 348), ('featur', 347), ('control', 337), ('key', 336), ('structur', 333), ('compar', 332), ('requir', 331), ('vision', 327), ('two', 324), ('task', 320), ('framework', 316), ('encrypt', 313), ('effect', 303), ('howev', 303), ('technolog', 302), ('oper', 298), ('code', 298), ('one', 295), ('test', 294), ('languag', 293), ('set', 281), ('achiev', 280), ('quantum', 276), ('queri', 274), ('dataset', 272), ('environ', 270), ('train', 269), ('measur', 267), ('visual', 264), ('challeng', 261), ('complex', 259), ('exist', 258), ('accuraci', 255), ('function', 255), ('architectur', 252), ('cryptographi', 250), ('integr', 241), ('increas', 240), ('experi', 239), ('demonstr', 239), ('map', 237), ('multipl', 235), ('reduc', 235), ('construct', 235), (\"'s\", 233), ('deep', 232), ('number', 231), ('user', 231), ('estim', 226), ('transform', 226), ('attack', 223)]\n"
     ]
    }
   ],
   "source": [
    "# 5. Check most frequent words - candidates to add to the stopword list\n",
    "listofall = [item for elem in stemmed_tokens_list for item in elem]\n",
    "\n",
    "freq = FreqDist(listofall)\n",
    "wnum = freq.B()\n",
    "print(\"\\nMost common words (total %d)\" % wnum)\n",
    "print(freq.most_common(100))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-24T14:41:01.255059100Z",
     "start_time": "2023-11-24T14:41:01.190650700Z"
    }
   },
   "id": "52351b4f14e0ca65"
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The tf-idf values of the first document\n",
      "\n",
      "use 0.018523277014819366\n",
      "paper 0.024194995885516137\n",
      "method 0.02762676250722815\n",
      "differ 0.030634027351767523\n",
      "also 0.030932104969452822\n",
      "howev 0.032481823161295754\n",
      "one 0.03406916359899369\n",
      "new 0.03473757250710102\n",
      "demonstr 0.036056357098364224\n",
      "compil 0.03670130394643328\n",
      "import 0.0390793750915981\n",
      "control 0.03964266735318102\n",
      "mani 0.0397250946264875\n",
      "introduc 0.039891477198720465\n",
      "sever 0.04120947950484785\n",
      "solut 0.041871631766058376\n",
      "measur 0.04246629369463887\n",
      "area 0.04319494775078536\n",
      "automat 0.04374064242487513\n",
      "may 0.04466249346927493\n",
      "execut 0.04539777492519779\n",
      "possibl 0.04578102778030854\n",
      "find 0.04685974871770827\n",
      "becom 0.048186407581572735\n",
      "help 0.04865932688305111\n",
      "base 0.04923899532537756\n",
      "system 0.05000446437729055\n",
      "call 0.050008133356129594\n",
      "toward 0.05036753644639746\n",
      "year 0.05055089733168141\n",
      "exampl 0.05111644117077749\n",
      "comput 0.052807365091497076\n",
      "run 0.05365078671383848\n",
      "cloud 0.05365078671383848\n",
      "amount 0.055122445607142107\n",
      "shown 0.0559232841806597\n",
      "benefit 0.0559232841806597\n",
      "remot 0.05737380296425813\n",
      "us 0.05768362849828257\n",
      "last 0.05800074470988467\n",
      "consumpt 0.05800074470988467\n",
      "usag 0.05899948430170992\n",
      "gener 0.06068480029021886\n",
      "process 0.061016541516042064\n",
      "etc 0.06209341707949437\n",
      "save 0.06299220454067204\n",
      "time 0.06327849548125465\n",
      "techniqu 0.06645317635665052\n",
      "alreadi 0.06869096900979671\n",
      "energy 0.07280654166898398\n",
      "energi consumpt 0.07280654166898398\n",
      "sometim 0.07380528126080924\n",
      "go 0.07380528126080924\n",
      "decid 0.07380528126080924\n",
      "lot 0.07488400219820895\n",
      "compil time 0.07605663861792525\n",
      "introduc new 0.07734111648593534\n",
      "batteri 0.07734111648593534\n",
      "task 0.07768997950947834\n",
      "mobil devic 0.07876104089277676\n",
      "last year 0.07876104089277676\n",
      "reduc 0.07879673581038159\n",
      "make 0.07978295439744093\n",
      "energy effici 0.0803483813304747\n",
      "weather 0.08214795949581753\n",
      "sport 0.08214795949581753\n",
      "job 0.08214795949581753\n",
      "system demonstr 0.08422542002504252\n",
      "system call 0.08422542002504252\n",
      "everyday 0.08422542002504252\n",
      "energi save 0.08422542002504252\n",
      "code 0.08433136615739806\n",
      "schedul techniqu 0.08668253431276889\n",
      "sever techniqu 0.08968979915730826\n",
      "process control 0.08968979915730826\n",
      "method comput 0.08968979915730826\n",
      "theori 0.0934401826628416\n",
      "theori base 0.09356683785187608\n",
      "part process 0.09356683785187608\n",
      "help us 0.09356683785187608\n",
      "effici schedul 0.09356683785187608\n",
      "part 0.09371949743541653\n",
      "effici 0.09715667931659851\n",
      "new system 0.09903121698414181\n",
      "mani area 0.09903121698414181\n",
      "control sever 0.09903121698414181\n",
      "decis 0.12015641047821929\n",
      "schedul 0.13108516874275078\n",
      "devic 0.13734308334092565\n",
      "code gener 0.13738193801959342\n",
      "decis make 0.14761056252161847\n",
      "energi effici 0.1606967626609494\n",
      "mobil 0.16309961435915626\n",
      "reduc energi 0.17937959831461653\n",
      "effici code 0.17937959831461653\n",
      "phone 0.18713367570375217\n",
      "life 0.193391590301927\n",
      "mobil phone 0.19806243396828363\n",
      "comput offload 0.2167452696219508\n",
      "smartphon 0.232023349457806\n",
      "energi 0.276920698000502\n",
      "offload 0.34673013725107554\n",
      "\n",
      "15000 unique features\n",
      "\n",
      "data exported to data.csv\n"
     ]
    }
   ],
   "source": [
    "# 6. Present as tf-idf\n",
    "cleaned_documents = [' '.join(i) for i in stemmed_tokens_list]\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 2),  # unigrams and bigrams\n",
    "                                   norm='l2',  # default\n",
    "                                   smooth_idf=False,  # was in the given preproccessing.py\n",
    "                                   max_df=0.9,\n",
    "                                   max_features=15000\n",
    "                                   )\n",
    "# only tf part:\n",
    "# tfidf_vectorizer = TfidfVectorizer(use_idf=False)\n",
    "\n",
    "tfidf_vectorizer.fit(cleaned_documents)\n",
    "tf_idf_vectors = tfidf_vectorizer.transform(cleaned_documents)\n",
    "\n",
    "print(\"\\nThe tf-idf values of the first document\\n\")\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "feature_index = tf_idf_vectors[3, :].nonzero()[1]\n",
    "tfidf_scores = list(zip(feature_index, [tf_idf_vectors[3, x] for x in feature_index]))\n",
    "tfidf_scores.sort(key=lambda t: t[1]) \n",
    "for w, s in [(feature_names[i], s) for (i, s) in tfidf_scores]:\n",
    "    print(w, s)\n",
    "\n",
    "print()\n",
    "# doesn't match the freq.B() before!! here it is 8k vs. 10k before. Not sure why\n",
    "print(len(feature_names), 'unique features')\n",
    "\n",
    "df = pd.DataFrame(list(tf_idf_vectors.toarray()))\n",
    "df.columns = feature_names\n",
    "df['_stemmed'] = cleaned_documents\n",
    "df['_title'] = titles  # with underscore to avoid coincidence with a 'title' feature\n",
    "\n",
    "# 7*. export data\n",
    "df.to_csv('data.csv', index=False)\n",
    "\n",
    "print('\\ndata exported to data.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-24T14:41:18.017009800Z",
     "start_time": "2023-11-24T14:41:01.255059100Z"
    }
   },
   "id": "b8f96ed3285320cc"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
